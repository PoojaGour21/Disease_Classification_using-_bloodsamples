# -*- coding: utf-8 -*-
"""Pooja_project(1)(blood samples).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rMNnhRZotwdzp6e-49-hkCZ13Ec0AAtR

#TABLE OF CONTENTS
#1.Loading librarys

#2.Reading the Dataset

#3.Data analysis

#4.Exploratory Data Analysis
   * Finding Null values in data set
   * Drop function

#5.Data visualization

   * Barplots
   * Pie plot

#6.multivariate analysis

   * Pair plots
   * Heatmaps
   * Box plots

#7.Encoding

   * Label encoding

#8.Feature Scaling

   * Standard Scaler

#9.Data spliting

   * spliting
   * Smote

#10.Algorithm

   * Random Forest

#11.classification models

   * Confusion matrix
   * classification report
   * fscores
   * Bar plot

#12.K-Nearest Neighbors (KNN)

#13.Additional Algorithm's

   * LogisticRegression
   * DecisionTreeClassifier
   * GaussianNB
   * SVM

#14.Feature Importance

   * Accuracy Score

#15.Cross Validation

#1.LOADING LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
# import plotly.express as px
# import plotly.figure_factory as ff
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,ConfusionMatrixDisplay
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import confusion_matrix,f1_score,accuracy_score

"""#2.READING THE DATASET"""

df=pd.read_csv('/content/blood_samples_dataset_test.csv')
df

"""# 3.DATA ANALYSIS

"""

df.head()

df.Disease.value_counts()

df.tail()

df.columns

df1=df.copy()

df.info()

df.describe()

df.sample()

df.dtypes

"""# 4.EDA (Exploratory Data Analysis)"""

df.isnull().sum()

"""**NOTE:** There are no null values present in the columns"""

# duplicate values
duplicate_rows = df[df.duplicated()].sum()
duplicate_rows

"""**NOTE**: There are no duplicate values .

#5.DATA VISUALIZATION

#BARPLOT
* A bar plot is a graphical representation of categorical data using rectangular bars whose lengths are proportional to the values they represent. The purpose of a bar plot is to visualize the distribution, comparison, or relationship between different categories or groups within a dataset. It provides a clear and concise way to display data
"""

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Disease').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""**NOTE**:compared to all diseases diabetes patients are more.

#PIE CHART
* pie chart is to visually represent the distribution of categorical data as proportions or percentages of a whole. It provides a clear and intuitive way to understand the relative sizes of different categories within a dataset, making it easier to identify patterns, trends, or disparities.
* Pie charts are especially useful for displaying data with a small number of categories and are commonly used in presentations, reports, and data visualization to communicate information effectively to a broad audience.
"""

plt.figure(figsize=(10, 10))
ct = pd.value_counts(df['Disease'].values, sort=False)
labels = ['Diabetes','Thalasse ','Heart Di','Thromboc','Healthy','Anemia ']
labels.sort()
sizes = ct
ct.sort_index(inplace=True)
colors = ['Red', 'pink', 'green', 'cyan','salmon','grey']
plt.pie(sizes, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=140)
plt.title('DISEASES').set_fontsize(21)
plt.axis('equal')
plt.show()

"""**NOTE**:According to pie chart the percentage of diabetes is high compared to all other diseases.

#6.MULTIVARIATE ANALYSIS

#HEATMAP
* **Visualizing relationships in tabular data**: Heatmaps are particularly useful for visualizing relationships in tabular data by representing each cell's value with a color. This allows us to quickly identify patterns, trends, and relationships within the data.

* **Correlation analysis**: Heatmaps are commonly used to visualize correlation matrices, where each cell represents the correlation coefficient between two variables. By using a color scale to represent the strength and direction of correlations, heatmaps make it easy to identify strong, weak, positive, and negative correlations.
"""

plt.figure(figsize=(25,35))
sns.heatmap(df.corr(),annot=True ,cmap="YlGnBu")

"""# BOX PLOT
* **Visualizing distribution**: Box plots provide a visual summary of the distribution of a continuous variable, including its central tendency, spread, and shape. The box in the plot represents the interquartile range (IQR), which contains the middle 50% of the data, while the whiskers extend to the minimum and maximum values within a certain range or are defined using statistical criteria.

* **Detecting skewness and outliers**: Box plots help identify skewness and outliers in the data. Skewness is evident if the median line is not centered within the box, while outliers are represented as individual data points beyond the whiskers of the plot. Outliers are important to identify as they can significantly affect statistical analyses and model performance.
"""

for column in df.columns:
    plt.figure(figsize=(6, 4))
    df.boxplot(column=[column])
    plt.title(f'Box Plot for {column}')
    plt.ylabel('Values')
    plt.show()

"""# PAIRPLOT
* A pair plot, also known as a scatterplot matrix, is a graphical tool used in data analysis to visualize the pairwise relationships between different variables in a dataset.
"""

sns.pairplot(df,hue='Disease')
plt.show()

"""#7.ENCODING

#LABEL ENCODING FOR CATEGORICAL COLUMNS
* label encoding is to convert categorical data into numerical format, assigning a unique numerical label to each category. This transformation enables machine learning algorithms to process categorical features, as they typically require numerical inputs.
* Label encoding is particularly useful for algorithms that can interpret ordinal relationships between categories, as it preserves the order of the labels. It also simplifies data preprocessing and reduces memory usage compared to other encoding techniques like one-hot encoding.
"""

from sklearn.preprocessing import LabelEncoder

# Assuming 'target_column' is the name of the target column in your dataset
label_encoder = LabelEncoder()

# Convert the 'Disease' column to numerical values
df['Disease'] = df['Disease'].astype('category')

# Fit the encoder to the target column
label_encoder.fit(df['Disease'])

# Transform the target column using the fitted encoder
df['Disease'] = label_encoder.transform(df['Disease'])

"""**NOTE**:Label Encoder is used to Convert the 'Disease' column to numerical values

#8.FEATURE SCALING
 **standard scaling**:
* **Normalize Data**: StandardScaler transforms features to have a mean of zero and a standard deviation of one, standardizing the scale across all features.
* **Enhance Algorithm Performance**: Standardized features improve the performance of machine learning algorithms, particularly those sensitive to variations in feature scales.
* **Facilitate Interpretability**: Standardization makes features easier to interpret as they are expressed in terms of standard deviations from the mean.
* **Ensure Algorithm Compatibility**: Many algorithms assume standardized features, making StandardScaler crucial for compatibility and convergence.
"""

#Standard Scaler
scaler = StandardScaler()
X_train = scaler.fit_transform(df.values.reshape(-1, 1))
X_test = scaler.transform(df.values.reshape(-1, 1))

"""#9.SPLITTING THE DATA
* The train_test_split function from scikit-learn is used to split a dataset into two subsets: one for training a machine learning model (x_train and y_train) and one for evaluating its performance (X_test and y_test)

* **Evaluation of model performance**: By splitting the dataset into training and testing subsets, we can evaluate the performance of the trained model on unseen data. The testing subset (X_test and y_test) serves as a proxy for real-world data, allowing us to assess the model's generalization ability and detect overfitting.
"""

X = df.drop('Disease', axis=1)
y = df['Disease']

"""**SMOTE FUNCTION**
 * The Synthetic Minority Over-sampling Technique (SMOTE) function is used to address class imbalance in datasets by generating synthetic samples for the minority class, thereby improving the classifier's performance by providing a more balanced training set.
"""

#Set the number of nearest neighbors to a value less than or equal to the number of samples
smote = SMOTE(k_neighbors=4)

# Resample the data using SMOTE
X, y = smote.fit_resample(X, y)

y.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

"""#10.ALGORITHMS

#RANDOM FOREST
* **High predictive accuracy**: Random Forest typically provides high predictive accuracy compared to many other algorithms. It works well with both categorical and numerical data and is less prone to overfitting, making it a popular choice for various predictive modeling tasks.

* **Ensemble learning**: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. Each decision tree is trained on a random subset of the training data and features, and predictions are made by aggregating the predictions of individual trees
"""

X_train_rfc = X_train.drop(columns = ['AST','Creatinine','LDL Cholesterol','Heart Rate'])

X_test_rfc = X_test.drop(columns = ['AST','Creatinine','LDL Cholesterol','Heart Rate'])

"""* NOTE: according  to the importance of the columns we dropped the four columns which are listed below:"""

#Random forest code
RFclassifier = RandomForestClassifier(n_estimators=1000,max_leaf_nodes=21)
RFclassifier.fit(X_train_rfc, y_train)

y_test_pred_rf = RFclassifier.predict(X_test_rfc)
y_train_pred_rf=RFclassifier.predict(X_train_rfc)

train_accuracy_rf=accuracy_score(y_train_pred_rf,y_train)
test_accuracy_rf=accuracy_score(y_test_pred_rf,y_test)

print(confusion_matrix(y_test,y_test_pred_rf))
print('training accuracy',accuracy_score(y_train,y_train_pred_rf))
print('testing accuracy',accuracy_score(y_test,y_test_pred_rf))
#print('training precion',precision_score(y_train,y_train_pred_rf))
#print('training precion',precision_score(y_test,y_test_pred_rf))

"""#11.CLASSIFICATION MODELS"""

# Create the feature importances Series
fscores = pd.Series(RFclassifier.feature_importances_, index=X_train_rfc.columns).sort_values(ascending=False)
fscores

"""# FEATURE IMPORTANCE(after dropping the columns)"""

#Using fscores finding the important column and plot using barplot
import plotly.express as px
fig = px.bar(fscores, x=fscores.values, y=fscores.index, orientation='h',
             title='Feature Importances', labels={'y': 'Feature', 'x': 'Importance'})

fig.show()

# classification Report
cr1=classification_report(y_test,y_test_pred_rf)
print(cr1)

"""#12.KNN(K-Nearest Neighbour)"""

# Knn code
from sklearn.neighbors import KNeighborsClassifier
knnclassifier=KNeighborsClassifier(n_neighbors=15,metric='minkowski')
knnclassifier.fit(X_train,y_train)

y_train_pred_knn=knnclassifier.predict(X_train)
y_test_pred_knn=knnclassifier.predict(X_test)

train_accuracy_knn=accuracy_score(y_train_pred_knn,y_train)
test_accuracy_knn=accuracy_score(y_test_pred_knn,y_test)

print('Training accuracy(knn): {:.2f}%'.format(train_accuracy_knn*100))
print('Testing accuracy(knn): {:.2f}%'.format(test_accuracy_knn*100))

print("\nConfusion Matrix (Testing Set):")
print(confusion_matrix(y_test, y_test_pred_knn))

# classification Report
cr2=classification_report(y_test,y_pred)
print(cr2)

"""#13.ADDITIONAL ALGORITHMS

#DECISION TREE
>
* Decision trees are easy to understand and interpret, making them valuable for gaining insights into the decision-making process of the model. The decision rules learned by the tree can be visualized, allowing users to comprehend how the model arrives at predictions.
"""

# Decision tree code
from sklearn.tree import DecisionTreeClassifier

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

train_pred=dt_model.predict(X_train)
y_pred = dt_model.predict(X_test)

train_accuracy_dt=accuracy_score(train_pred,y_train)
test_accuracy_dt=accuracy_score(y_pred,y_test)

print(confusion_matrix(y_test,y_pred))
print('training accuracy',accuracy_score(y_train,train_pred))
print('testing accuracy',accuracy_score(y_test,y_pred))

# classification Report
cr3=classification_report(y_test,y_pred)
print(cr3)

"""#LOGISTIC REGRESSION



* Logistic Regression is a powerful and versatile tool for binary
  classification tasks, particularly when interpretability, speed, and efficiency are important considerations. However, it's essential to note that Logistic Regression has limitations, such as its inability to capture complex nonlinear relationships in the data, which may require more sophisticated models for certain tasks.






"""

# Logistic regression code
from sklearn.linear_model import LogisticRegression

LogRegClassifier = LogisticRegression(max_iter=500)
LogRegClassifier.fit(X_train, y_train)

y_train_pred_logreg = LogRegClassifier.predict(X_train)
y_test_pred_logreg = LogRegClassifier.predict(X_test)


train_accuracy_logreg = accuracy_score(y_train_pred_logreg, y_train)
train_f1_score_logreg = f1_score(y_train, y_train_pred_logreg, average='weighted')
test_accuracy_logreg = accuracy_score(y_test_pred_logreg, y_test)
test_f1_score_logreg = f1_score(y_test, y_test_pred_logreg, average='weighted')


print('Training Accuracy (Logistic Regression): {:.2f}%'.format(train_accuracy_logreg * 100))
print('Testing Accuracy (Logistic Regression): {:.2f}%'.format(test_accuracy_logreg * 100))
print('Training F1 Score (Logistic Regression): {:.2f}'.format(train_f1_score_logreg*100))
print('Testing F1 Score (Logistic Regression): {:.2f}'.format(test_f1_score_logreg*100))


print("\nConfusion Matrix (Testing Set):")
print(confusion_matrix(y_test, y_test_pred_logreg))

# classification Report
cr4=classification_report(y_test,y_pred)
print(cr4)

"""# NAIVE BAYES

* Gaussian Naive Bayes (GaussianNB) is a variant of the Naive Bayes algorithm

* **Simplicity**: GaussianNB is a simple and easy-to-understand classification algorithm. It's straightforward to implement, making it a good choice for quick
prototyping or as a baseline model.

* **Efficiency**: GaussianNB is computationally efficient, making it suitable for large datasets. It scales well with the number of features and samples, and the time complexity for training and prediction is linear with respect to the number of features and samples.



"""

# naive bayes code
from sklearn.naive_bayes import GaussianNB
Naiveclassifier=GaussianNB()
Naiveclassifier.fit(X_train,y_train)

y_train_pred_Naive=Naiveclassifier.predict(X_train)
y_test_pred_Naive=Naiveclassifier.predict(X_test)

train_accuracy_Naive=accuracy_score(y_train_pred_Naive,y_train)
test_accuracy_Naive=accuracy_score(y_test_pred_Naive,y_test)

print('Training accuracy(Naive Bayes): {:.2f}%'.format(train_accuracy_Naive*100))
print('Testing accuracy(Naive Bayes): {:.2f}%'.format(test_accuracy_Naive*100))

print("\nConfusion Matrix (Testing Set):")
print(confusion_matrix(y_test, y_test_pred_Naive))

# classification Report
cr5=classification_report(y_test,y_pred)
print(cr5)

"""#SVM(support vector machine)
>* Support Vector Machine (SVM) is a general term that refers to the algorithm used for both classification and regression tasks. Support Vector Classification (SVC), on the other hand, specifically refers to SVM when applied to classification problems.

>* In essence, SVC is a type of SVM tailored for classification tasks. It aims to find the hyperplane that best separates different classes in the feature space, maximizing the margin between the classes. SVC is widely used for binary and multi-class classification problems due to its effectiveness in handling both linearly separable and non-linearly separable data.





"""

from sklearn.svm import SVC
svmclassifier=SVC(kernel='rbf',max_iter=500)
svmclassifier.fit(X_train,y_train)

y_train_pred_svm=svmclassifier.predict(X_train)
y_test_pred_svm=svmclassifier.predict(X_test)

train_accuracy_svm=accuracy_score(y_train_pred_svm,y_train)
test_accuracy_svm=accuracy_score(y_test_pred_svm,y_test)

print('Training accuracy(SVM): {:.2f}%'.format(train_accuracy_svm*100))
print('Testing accuracy(SVM): {:.2f}%'.format(test_accuracy_svm*100))

print("\nConfusion Matrix (Testing Set):")
print(confusion_matrix(y_test, y_test_pred_svm))

# classification Report
cr6=classification_report(y_test,y_pred)
print(cr6)

"""#14.FEATURE IMPORTANCE
# BARGRAPH FOR ACCURACYSCORE


"""

plt.figure(figsize=(10, 3))
plt.bar(x=['Naiveclassifier','knnclassifier','LogRegClassifier ','dt_model','svmclassifier','RFclassifier'],height=[test_accuracy_svm,test_accuracy_knn,test_accuracy_Naive,test_accuracy_rf,test_accuracy_logreg,test_accuracy_dt],color='#00FF00')
plt.xlabel('Algorithms',fontsize=20)
plt.ylabel('Accuracy Score',fontsize=20)
plt.show()

"""#15.CROSS VALIDATION"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import

# Random Forest
rf=RandomForestClassifier(n_estimators=50,criterion='entropy',max_depth=5)
print(cross_val_score(rf,X,y,cv=10,scoring='accuracy').mean())

# Decision Tree
dt=DecisionTreeClassifier(max_depth=5,criterion='gini',max_features=6)
print(cross_val_score(dt,X,y,cv=10,scoring='accuracy').mean())

# Logistic Regression
logreg=LogisticRegression()
print(cross_val_score(logreg,X,y,cv=10,scoring='accuracy').mean())

# Naive Bayes
Naiveclassifier=GaussianNB()
print(cross_val_score(Naiveclassifier,X,y,cv=10,scoring='accuracy').mean())

# KNN
knn=KNeighborsClassifier(n_neighbors=4)
print(cross_val_score(knn,X,y,cv=10,scoring='accuracy').mean())

#SVM
svmclassifier=SVC(kernel='rbf',max_iter=500)
print(cross_val_score(svmclassifier,X,y,cv=10,scoring='accuracy').mean())